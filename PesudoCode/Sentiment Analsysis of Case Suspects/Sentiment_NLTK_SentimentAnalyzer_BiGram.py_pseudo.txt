

IMPORT random

IMPORT pandas as pd

IMPORT pyodbc

from unidecode IMPORT unidecode

from nltk.classify IMPORT NaiveBayesClassifier

from nltk.sentiment IMPORT SentimentAnalyzer

from nltk.sentiment.util IMPORT mark_negation, extract_bigram_feats

IMPORT re, string

from nltk.tag IMPORT pos_tag

from nltk.stem.wordnet IMPORT WordNetLemmatizer

from nltk.tokenize IMPORT word_tokenize # tokenize words

from datetime IMPORT datetime

from nltk IMPORT sent_tokenize # tokenize sentences



SET t TO str(datetime.today().year) + str(datetime.today().month)+ str(datetime.today().day)+"_"+str(datetime.today().hour)+"_"+str(datetime.today().minute)+"_"+str(datetime.today().second)

OUTPUT(t)

OUTPUT("Program to ascertain sentiment polarity using BiGrams")

OUTPUT("Reading DB connection parameters from file...")

# Read DB connection parameters from file

SET f TO open(r"F:\Dissertation\Req 3.1 & 3.2 Sentiment Analysis\Python\DBConn.txt","r")

SET server TO f.readline().rstrip() # remove any line breaks at the end of the line

SET DB TO f.readline().rstrip() # remove any line breaks at the end of the line

f.close()

OUTPUT("File read complete")

OUTPUT("Opening DB Connection...")

SET conn TO pyodbc.connect('Driver={SQL Server Native Client 11.0};Server='+ server +';Database=' + DB +';Trusted_Connection=yes;')

SET cursor TO conn.cursor()

OUTPUT("DB Connection successfully opened")

# Fetch into pandas dataframe via a select statement

SET df_sql_data TO pd.read_sql_query("SELECT [sentiment],[review] FROM [MasterDB].[dbo].[SentiWordnet_labeledTrainData_stanford]",conn)

SET #data TO pd.read_csv(r"F:\Dissertation\Req 3.1 & 3.2 Sentiment Analysis\Python\SentiWordnet_labeledTrainData.tsv", header=0, delimiter="\t", quoting=3)

OUTPUT("Fetch from TBL MasterDB.dbo.SentiWordnet_labeledTrainData_stanford completed")

SET sentiment_data TO list(zip(df_sql_data["review"], df_sql_data["sentiment"]))



#ignoring as repeated runs FOR a cse ESI analytics can cause changing results. This could be toggled IF needed on the INPUT parameters of the python script.

#random.shuffle(sentiment_data)

SET OUTPUT('TRAIN SET TO Top 5000 rows')

SET train_X, train_y TO list(zip(*sentiment_data[:5000])) # gives top 5K. Increasing training reduces accuracy.. see notes on folder


SET OUTPUT('TEST SET TO Bottom 1000 rows')

SET test_X, test_y TO list(zip(*sentiment_data[24000:])) # gives bottom 1K



SET def remove_noise(sentance_tokens, stop_words TO ()):

    SET cleaned_tokens TO []

    FOR token, tag IN pos_tag(sentance_tokens):

        SET token TO re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\(\),]|'

                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)

        SET token TO re.sub("(@[A-Za-z0-9_]+)","", token)



        IF tag.startswith("NN"):

            SET pos TO 'n'

        ELSEIF tag.startswith('VB'):

            SET pos TO 'v'

        ELSE:

            SET pos TO 'a'



        SET lemmatizer TO WordNetLemmatizer()

        SET token TO lemmatizer.lemmatize(token, pos)



        IF len(token) > 0 and token not IN string.punctuation and token.lower() not IN stop_words:

            cleaned_tokens.append(token.lower())

    RETURN cleaned_tokens



DEFINE FUNCTION clean_text(text):

    SET text TO text.replace("<br />", " ")

    SET #text TO text.decode("utf-8") // not required IN Python 3

    RETURN text



SET analyzer TO SentimentAnalyzer()

# mark_negation appends a "_NEG" to words after a negation untill a punctuation mark. This means that the same after a negation will be handled differently

# than the word that's not after a negation by the classifier

SET vocabulary TO analyzer.all_words([mark_negation(word_tokenize(unidecode(clean_text(instance))))

                                 FOR instance IN train_X])

OUTPUT("Vocabulary: ", len(vocabulary))

# In Bigram we assume that the occurrence of each word is independent of its previous word.

OUTPUT("Computing Bigram Features ...")

SET bigram_features TO analyzer.bigram_collocation_feats(vocabulary) #Returns most common top_n word features.

OUTPUT("Bigram Features: ", len(bigram_features))

# extract_Bigram_feats - Populates a dictionary of Bigram features, reflecting the presence/absence IN the document of each of the tokens IN Bigrams.

analyzer.add_feat_extractor(extract_bigram_feats, bigrams=bigram_features) #Add a new function extract_Bigram_feats to extract features from a document.



# Apply the features to obtain a feature-value representation of our datasets:

# training set

SET _train_X TO analyzer.apply_features([mark_negation(remove_noise(word_tokenize(unidecode(clean_text(instance.lower())))))

                                     FOR instance IN train_X], labeled=False) #Apply all feature extractor functions to the documents.

# test set

SET _test_X TO analyzer.apply_features([mark_negation(remove_noise(word_tokenize(unidecode(clean_text(instances.lower())))))

                                   FOR instances IN test_X], labeled=False) #Apply all feature extractor functions to the documents.



# We can now train our classifier on the training set, and subsequently output the evaluation results:

SET trainer TO NaiveBayesClassifier.train

SET classifier TO analyzer.train(trainer, list(zip(_train_X, train_y)))

classifier.show_most_informative_features(10)

SET score TO analyzer.evaluate(list(zip(_test_X, test_y)))

OUTPUT("Analyzer Evaluation results: ", score)

OUTPUT("Model Accuracy: ", score['Accuracy']) # 75.4% FOR 500, 62% FOR 50, 81.8% FOR 5000

model_accuracy=round(score['Accuracy']*100)



IMPORT sklearn.metrics as metrics

IMPORT matplotlib.pyplot as plt

y_score=[]

FOR row IN test_X:

    SET tokens TO mark_negation(remove_noise(word_tokenize(unidecode(clean_text(row.lower())))))

    #OUTPUT(classifier.classify(dict([token, True] FOR token IN tokens)))

    y_score.append(classifier.classify(dict([token, True] FOR token IN tokens)))


# Plot Graphs using matplotlib.pyplot

# START OF DB TRANSACTIONS - RUN Model against case ESI TWitter, SMS/WHATSAPP, MSWord, Facebook and emails and store results in appropriate SQL tables)





