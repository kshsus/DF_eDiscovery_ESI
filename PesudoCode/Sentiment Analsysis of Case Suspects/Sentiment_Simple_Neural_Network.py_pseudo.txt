# Import Python Libraries

OUTPUT("Program to ascertain sentiment using a Simple Neural Network")

OUTPUT("Reading DB connection parameters from file...")

# Read DB connection parameters from file

OUTPUT("Opening DB Connection...")

# Fetch into pandas dataframe via a select statement

SET #data TO pd.read_csv(SentiWordnet_labeledTrainData.tsv, header=0, delimiter="\t", quoting=3)

SET maxlen TO 100

SET TAG_RE TO re.compile(r'<[^>]+>')



DEFINE FUNCTION remove_tags(text):

        SET text TO text.replace("<br />", " ")

        SET # text TO text.decode("utf-8") // not required IN Python 3

        RETURN TAG_RE.sub('', text)



DEFINE FUNCTION preprocess_text(sen):
    SET sentence TO remove_tags(sen)
    SET sentence TO re.sub('[^a-zA-Z]', ' ', sentence)
    SET sentence TO re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)
    SET sentence TO re.sub(r'\s+', ' ', sentence)
    RETURN sentence

SET X TO []

SET sentences TO list(df_sql_data['review'].values)

FOR sen IN sentences:

    X.append(preprocess_text(sen))

SET y TO df_sql_data['sentiment'].values

SET X_train, X_test, y_train, y_test TO train_test_split(X, y, test_size=0.20, random_state=42)


# Preparing the Embedding Layer

SET tokenizer TO Tokenizer(num_words=5000)

tokenizer.fit_on_texts(X_train)



SET X_train TO tokenizer.texts_to_sequences(X_train)

SET X_test TO tokenizer.texts_to_sequences(X_test)

SET vocab_size TO len(tokenizer.word_index) + 1

SET X_train TO pad_sequences(X_train, padding='post', maxlen=maxlen)

SET X_test TO pad_sequences(X_test, padding='post', maxlen=maxlen)

SET embeddings_dictionary TO dict()

SET glove_file TO open(glove_6B_100d.txt, encoding="utf8")


FOR line IN glove_file:
    SET records TO line.split()
    SET word TO records[0]
    SET vector_dimensions TO asarray(records[1:], dtype='float32')
    SET embeddings_dictionary [word] TO vector_dimensions

glove_file.close()



# Create an embedding matrix with 100 columns where each column will contain the GloVe word embeddings FOR the words IN our corpus.


SET embedding_matrix TO zeros((vocab_size, 100))

FOR word, index IN tokenizer.word_index.items():

    SET embedding_vector TO embeddings_dictionary.get(word)

    IF embedding_vector is not None:

        SET embedding_matrix[index] TO embedding_vector



# Text Classification with Simple Neural Network


SET model TO Sequential() # we create a Sequential() model.

SET embedding_layer TO Embedding(vocab_size, 100, weights=[embedding_matrix], INPUT_length=maxlen , trainable=False)

model.add(embedding_layer) 

model.add(Flatten()) 

model.add(Dense(1, activation='sigmoid')) # Finally, we add a dense layer with sigmoid activation function.

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

OUTPUT(model.summary())



# Use the fit method to train our neural network

SET history TO model.fit(X_train, y_train, batch_size=10, epochs=10, verbose=1, validation_split=0.2)


# Evaluate the performance of the model

SET train_score TO model.evaluate(X_train, y_train, verbose=1)

OUTPUT("Train Score:", train_score[0])

OUTPUT("Train Accuracy:", train_score[1])

SET test_score TO model.evaluate(X_test, y_test, verbose=1)

OUTPUT("Test Score:", test_score[0])

OUTPUT("Test Accuracy:", test_score[1])


# Plot Graphs using matplotlib.pyplot
# START OF DB TRANSACTIONS - RUN Model against case ESI TWitter, SMS/WHATSAPP, MSWord, Facebook and emails and store results in appropriate SQL tables)

