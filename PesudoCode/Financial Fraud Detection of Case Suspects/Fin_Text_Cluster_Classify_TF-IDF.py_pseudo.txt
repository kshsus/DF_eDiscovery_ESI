from datetime IMPORT datetime

IMPORT pyodbc

from sklearn.feature_extraction.text IMPORT TfidfVectorizer


IMPORT json

IMPORT pandas as pd

IMPORT numpy as np

from sklearn IMPORT feature_selection

from sklearn IMPORT metrics



IMPORT matplotlib.pyplot as plt

IMPORT seaborn as sns


IMPORT re

IMPORT nltk



## FOR bag-of-words

from sklearn IMPORT feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing



## FOR word embedding

IMPORT gensim

IMPORT gensim.downloader as gensim_api






OUTPUT("Program to find buy/sell of shares/stock intent")

OUTPUT("Reading DB connection parameters from file...")

# Read DB connection parameters from file

SET f TO open(r"DBConn.txt","r")

SET server TO f.readline().rstrip() # remove any line breaks at the end of the line

SET ESI_DB TO f.readline().rstrip() # remove any line breaks at the end of the line

SET Master_DB TO f.readline().rstrip() # remove any line breaks at the end of the line

f.close()

OUTPUT("File read complete")



OUTPUT("Opening ESI DB Connection...")

SET conn1 TO pyodbc.connect('Driver={SQL Server Native Client 11.0};Server='+ server +';Database=' + ESI_DB +';Trusted_Connection=yes;')

SET cursor1 TO conn1.cursor()

SET conn2 TO pyodbc.connect('Driver={SQL Server Native Client 11.0};Server='+ server +';Database=' + Master_DB +';Trusted_Connection=yes;')

SET cursor2 TO conn2.cursor()

OUTPUT("DB Connection successfully opened")

# Fetch into pandas dataframe via a select statement

SET Qery_str TO "SELECT [text] ,[BERT_Intent] as y FROM .."

SET df_ESI_sql_data TO pd.read_sql_query(Qery_str,conn2)

OUTPUT("Fetch from MasterDB completed FOR text to be processed")



## OUTPUT 5 random rows

OUTPUT(df_ESI_sql_data.sample(5))



'''

Preprocess a string.

:parameter

    :param text: string - name of column containing text

    :param lst_stopwords: list - list of stopwords to remove

    :param flg_stemm: bool - whether stemming is to be applied

    :param flg_lemm: bool - whether lemmitisation is to be applied

:RETURN

    cleaned text

'''





DEFINE FUNCTION utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):
    SET text TO re.sub(r'[^\w\s]', '', str(text).lower().strip())

    SET lst_text TO text.split()

    IF lst_stopwords is not None:

        SET lst_text TO [word FOR word IN lst_text IF word not IN lst_stopwords]

    IF flg_stemm EQUALS True:

        SET ps TO nltk.stem.porter.PorterStemmer()

        SET lst_text TO [ps.stem(word) FOR word IN lst_text]


    IF flg_lemm EQUALS True:

        SET lem TO nltk.stem.wordnet.WordNetLemmatizer()

        SET lst_text TO [lem.lemmatize(word) FOR word IN lst_text]


    SET text TO " ".join(lst_text)

    RETURN text



SET lst_stopwords TO nltk.corpus.stopwords.words("english")

OUTPUT(lst_stopwords)



SET df_ESI_sql_data["text_clean"] TO df_ESI_sql_data["text"].apply(lambda x:

          utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,

          lst_stopwords=lst_stopwords))

OUTPUT(df_ESI_sql_data.head())



## split dataset

SET dtf_train, dtf_test TO model_selection.train_test_split(df_ESI_sql_data, test_size=0.3)


SET y_train TO dtf_train["y"].values

SET y_test TO dtf_test["y"].values


## Count (classic BoW)

SET #vectorizer TO feature_extraction.text.CountVectorizer(max_features=10000, ngram_range=(1,2))



## Tf-Idf (advanced variant of BoW)

SET vectorizer TO feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))



SET corpus TO dtf_train["text_clean"]

vectorizer.fit(corpus)

SET X_train TO vectorizer.transform(corpus)

SET dic_vocabulary TO vectorizer.vocabulary_

# To Overcome TypeError: Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type

# There is a unfortuate incompatibility with old pandas and 1.20. Updating pandas to a newer version should fix it,

# Updating to pandas>=1.0.5 should solve it. Supposedly also pandas==0.25.3 works.

# If you are stuck with pandas 0.24.x you may not be able to usse numpy 1.20.x though, unfortunaly. In that case, use numpy<1.20,

# Pandas also provides a utility function, pd.show_versions(), which reports the version of its dependencies as well:

# pip3 install --upgrade pandas or conda update pandas



SET y TO dtf_train["y"]

SET X_names TO vectorizer.get_feature_names_out()

SET p_value_limit TO 0.95

SET dtf_features TO pd.DataFrame()



FOR cat IN np.unique(y):

    SET chi2, p TO feature_selection.chi2(X_train, y==cat)

    SET dtf_features TO dtf_features.append(pd.DataFrame(

                   {"feature":X_names, "score":1-p, "y":cat}))

    SET dtf_features TO dtf_features.sort_values(["y","score"], ascending=[True,False])

    SET dtf_features TO dtf_features[dtf_features["score"] > p_value_limit]



SET X_names TO dtf_features["feature"].unique().tolist()

SET OUTPUT('X_names TO ',X_names)



SET vectorizer TO feature_extraction.text.TfidfVectorizer(vocabulary=X_names)

vectorizer.fit(corpus)

SET X_train TO vectorizer.transform(corpus)

SET dic_vocabulary TO vectorizer.vocabulary_



SET classifier TO naive_bayes.MultinomialNB()



## pipeline

SET model TO pipeline.Pipeline([("vectorizer", vectorizer),

                           ("classifier", classifier)])## train classifier

model["classifier"].fit(X_train, y_train)## test

SET X_test TO dtf_test["text_clean"].values

SET predicted TO model.predict(X_test)

SET predicted_prob TO model.predict_proba(X_test)



SET classes TO np.unique(y_test)

SET y_test_array TO pd.get_dummies(y_test, drop_first=False).values





## Accuracy, Precision, Recall

## Plot confusion matrix

## Plot roc

## Plot precision-recall curve





OUTPUT("=========== START OF Housekeeping DB TRANSACTIONS ======================================")

OUTPUT("Now let's run the various subReddits through this BERT program and insert results into BERT_Master_praw_Reddit .. ")

OUTPUT("Backup tables before fresh insert")

OUTPUT("=========== END OF Housekeeping DB TRANSACTIONS ======================================")

OUTPUT("Start of INSERTS ======================================")



# Predict text of ESI and store IN a table



OUTPUT("\n")

OUTPUT("Prediction")

SET df_ESI_sql_data TO pd.read_sql_query("SELECT [Fin_Texts_4_Snips_ID], [BatesID], [Txt], [Actor], [Stock_Ticker] FROM [" + ESI_DB +"].[dbo].[Fin_Texts_4_SnipsIntent_Senti_Pattern]", conn1)



SET for row IN df_ESI_sql_data.itertuples(index TO True, name ='Pandas'):

    SET pred_class  TO model.predict([row[3]]) # which one??? Buy, Other, Sell

    SET predicted_prob TO model.predict_proba([row[3]]) # IN the order of Buy, Other, Sell

    OUTPUT(row[3], pred_class , predicted_prob)

    SET pred_class TO ' :: '.join(str(v) FOR v IN pred_class)

    SET predicted_classes_prob TO ' :: '.join(str(v) FOR v IN predicted_prob) # IN the order of Buy, Other, Sell

    SET sql TO "INSERT INTO [" + ESI_DB +"].[dbo].[Fin_TfIdf_Intent] ([Origin_Tbl_BatesID],[text],[TfIdf_Intent_Cluster_No]," \

                                      "[TfIdf_Predicted_Intent_Similarities], [Model_Accuracy]) VALUES (?,?,?,?,?)"

    OUTPUT(sql, ('TBL::Fin_Texts_4_SnipsIntent_Senti_Pattern|'+str(row[1])+'|'+str(row[2]), row[3], pred_class ,predicted_classes_prob, round(accuracy, 2)))

    cursor1.execute(sql, ('TBL::Fin_Texts_4_SnipsIntent_Senti_Pattern|'+str(row[1])+'|'+str(row[2]), row[3], pred_class ,predicted_classes_prob, round(accuracy, 2)))

    conn1.commit()


